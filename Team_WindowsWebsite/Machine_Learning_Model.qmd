---
title: "Machine Learning Model"
format: html
editor: visual
---

## Description
The goal of this section is to demonstrate whether or not our statistic for wind categorization can be predicted using a machine learning model. We chose a random forest model, as our variable is categorical with more than two choices. In building the model, we tuned the following hyperparameters using cross validation: number of trees, minimum samples required to be at a leaf node, and the number of features to consider at each split in a tree. During this hyperparameter tuning step, which is typically time consuming for large datasets such as ours, we used parallel computing. Future directions may consider different machine learning models to improve accuracy.


```{r}
library(dplyr)
library(tidymodels)
library(ranger)
library(parallel)
library(future)
```

### Preparing the Data

```{r}
# Read in data
ML_data <- read.csv("ML_wind_data.csv")

# Remove variables in aggregate statistic
ML_data <- ML_data %>% select(-INJURIES_DIRECT, -INJURIES_INDIRECT, -DEATHS_DIRECT, -DEATHS_INDIRECT, -DAMAGE_PROPERTY, -DAMAGE_CROPS, -aggregate_stat)

# Prepare remaining data for analysis -- make sure character strings are factors, etc.
ML_data <- ML_data %>% mutate(STATE = as.factor(STATE), YEAR = as.factor(YEAR), MONTH_NAME = as.factor(MONTH_NAME), EVENT_TYPE = as.factor(EVENT_TYPE), MAGNITUDE_TYPE = as.factor(MAGNITUDE_TYPE), scaled_aggregate = as.factor(scaled_aggregate))

ML_data <- na.omit(ML_data)
head(ML_data)
```

### Split the Data and Define the Model

```{r}
# Split data into training/test sets
set.seed(1234)
data_split <- initial_split(ML_data, prop = 3/4)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Define Recipe
ML_recipe <- recipe(scaled_aggregate ~ ., data = train_data) %>% update_role(EVENT_ID, new_role = "ID")

# Define Random Forest Model Specs
model_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "permutation")

# Create Workflow
ML_workflow <- workflow() %>%
  add_recipe(ML_recipe) %>%
  add_model(model_spec)
```

### Hyperparameter Tuning (with parallel computing)

```{r}
# Set up parallel computing
numCores <- detectCores()
plan(multisession, workers = numCores)

# Run hyperparameter tuning
## define cross validation folds
cv_folds <- vfold_cv(train_data, v = 3)

## define hyperparameters to tune
parameter_grid <- grid_regular(
  mtry(range = c(2, 7)),
  trees(range = c(50, 250)),
  min_n(range = c(1, 10))
)

## tune hyperparameters
tuned_results <- ML_workflow %>%
  tune_grid(resamples = cv_folds, grid = parameter_grid) 

tuned_results %>%
  collect_metrics()

# Return to work on one core
plan(sequential)
```

### Train and Test Model

```{r}
# Finalize workflow with best hyperparameters
best_params <- tuned_results %>%
  select_best(metric = "roc_auc")
best_params

ML_workflow_final <- ML_workflow %>% finalize_workflow(best_params)

# Train Data
trained_model <- ML_workflow_final %>%
  fit(data = train_data)

# Test data on testing data
test_predict <- predict(trained_model, new_data = test_data) %>%
  bind_cols(test_data)
```

### Evaluating the Model

```{r}
# Evaluate model
model_eval <- test_predict %>%
  metrics(truth = scaled_aggregate, estimate = .pred_class)

model_eval

# Feature Importance
importance <- trained_model %>%
  extract_fit_engine() %>%
  ranger::importance()

importance
```

## Conclusion
In the end, our model with the best hyperparameters had an accuracy of 0.6898446 and a kappa value – which measures how much better our model is than random chance (on a scale from 0 to 1) – of 0.4593361. The most important features in the model were wind magnitude and the state the event took place in. The least important features were changes in latitude and longitude, but that could be due to the fact that many wind events did not record changes in latitude and longitude. Because our accuracy and kappa values were higher than we would expect by random chance, we have shown that our statistic for wind categorization is predictable and future directions may be pursued.

